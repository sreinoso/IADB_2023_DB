{
  "paragraphs": [
    {
      "text": "%md\nLectura y escritura de ficheros\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 4:37:26 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eLectura y escritura de ficheros\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490983056725_1753162031",
      "id": "20170331-175736_1251486193",
      "dateCreated": "Mar 31, 2017 5:57:36 PM",
      "dateStarted": "Jul 15, 2017 11:37:54 AM",
      "dateFinished": "Jul 15, 2017 11:37:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Sistemas de ficheros soportados\n-   Igual que Hadoop, Spark soporta diferentes filesystems: local, HDFS, Amazon S3\n\n    -   En general, soporta cualquier fuente de datos que se pueda leer con Hadoop\n\n-   También, acceso a bases de datos relacionales o noSQL\n\n    -   MySQL, Postgres, etc. mediante JDBC\n    -   Apache Hive, HBase, Cassandra o Elasticsearch\n\n### Formatos de fichero soportados\n\n-   Spark puede acceder a diferentes tipos de ficheros:\n\n    -   Texto plano, CSV, ficheros sequence, JSON, *protocol buffers* y *object files*\n        -   Soporta ficheros comprimidos\n    -   Veremos el acceso a algunos tipos en esta clase, y dejaremos otros para más adelante \n",
      "user": "anonymous",
      "dateUpdated": "Jul 28, 2017 3:13:28 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSistemas de ficheros soportados\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eIgual que Hadoop, Spark soporta diferentes filesystems: local, HDFS, Amazon S3\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003eEn general, soporta cualquier fuente de datos que se pueda leer con Hadoop\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eTambién, acceso a bases de datos relacionales o noSQL\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eMySQL, Postgres, etc. mediante JDBC\u003c/li\u003e\n      \u003cli\u003eApache Hive, HBase, Cassandra o Elasticsearch\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eFormatos de fichero soportados\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eSpark puede acceder a diferentes tipos de ficheros:\n    \u003cul\u003e\n      \u003cli\u003eTexto plano, CSV, ficheros sequence, JSON, \u003cem\u003eprotocol buffers\u003c/em\u003e y \u003cem\u003eobject files\u003c/em\u003e\n        \u003cul\u003e\n          \u003cli\u003eSoporta ficheros comprimidos\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eVeremos el acceso a algunos tipos en esta clase, y dejaremos otros para más adelante\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500118674272_-801862945",
      "id": "20170715-113754_1944635351",
      "dateCreated": "Jul 15, 2017 11:37:54 AM",
      "dateStarted": "Jul 17, 2017 5:01:41 PM",
      "dateFinished": "Jul 17, 2017 5:01:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n#### Ejemplos con ficheros de texto\n\nEn el directorio `../datos/libros` hay un conjunto de ficheros de texto comprimidos.",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 4:37:32 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eEjemplos con ficheros de texto\u003c/h4\u003e\n\u003cp\u003eEn el directorio \u003ccode\u003e../datos/libros\u003c/code\u003e hay un conjunto de ficheros de texto comprimidos.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500118745953_-1002008994",
      "id": "20170715-113905_678503000",
      "dateCreated": "Jul 15, 2017 11:39:05 AM",
      "dateStarted": "Jul 15, 2017 12:26:46 PM",
      "dateFinished": "Jul 15, 2017 12:26:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n# Ficheros de entrada\nls ../datos/libros",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:11:18 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500118790611_1294385336",
      "id": "20170715-113950_1612698555",
      "dateCreated": "Jul 15, 2017 11:39:50 AM",
      "dateStarted": "Jul 17, 2017 5:11:18 PM",
      "dateFinished": "Jul 17, 2017 5:11:19 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Funciones de lectura y escritura con ficheros de texto\n\n\n- `sc.textFile(nombrefichero/directorio)` Crea un RDD a partir las líneas de uno o varios ficheros de texto\n    - Si se especifica un directorio, se leen todos los ficheros del mismo, creando una partición por fichero\n    - Los ficheros pueden estar comprimidos, en diferentes formatos (gz, bz2,...)\n    - Pueden especificarse comodines en los nombres de los ficheros\n- `sc.wholeTextFiles(nombrefichero/directorio)` Lee ficheros y devuelve un RDD clave/valor\n    - clave: path completo al fichero\n    - valor: el texto completo del fichero\n- `rdd.saveAsTextFile(directorio_salida)` Almacena el RDD en formato texto en el directorio indicado\n    - Crea un fichero por partición del rdd\n",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 4:37:35 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFunciones de lectura y escritura con ficheros de texto\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ccode\u003esc.textFile(nombrefichero/directorio)\u003c/code\u003e Crea un RDD a partir las líneas de uno o varios ficheros de texto\n    \u003cul\u003e\n      \u003cli\u003eSi se especifica un directorio, se leen todos los ficheros del mismo, creando una partición por fichero\u003c/li\u003e\n      \u003cli\u003eLos ficheros pueden estar comprimidos, en diferentes formatos (gz, bz2,\u0026hellip;)\u003c/li\u003e\n      \u003cli\u003ePueden especificarse comodines en los nombres de los ficheros\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003esc.wholeTextFiles(nombrefichero/directorio)\u003c/code\u003e Lee ficheros y devuelve un RDD clave/valor\n    \u003cul\u003e\n      \u003cli\u003eclave: path completo al fichero\u003c/li\u003e\n      \u003cli\u003evalor: el texto completo del fichero\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ccode\u003erdd.saveAsTextFile(directorio_salida)\u003c/code\u003e Almacena el RDD en formato texto en el directorio indicado\n    \u003cul\u003e\n      \u003cli\u003eCrea un fichero por partición del rdd\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500120467003_287319032",
      "id": "20170715-120747_1605706098",
      "dateCreated": "Jul 15, 2017 12:07:47 PM",
      "dateStarted": "Jul 17, 2017 4:14:15 PM",
      "dateFinished": "Jul 17, 2017 4:14:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lee todos los ficheros del directorio\n# y crea un RDD con las líneas\nlineas \u003d sc.textFile(\"../datos/libros\")\n\n# Se crea una partición por fichero de entrada\nprint(\"Número de particiones del RDD lineas \u003d {0}\".format(\n       lineas.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jul 23, 2017 3:49:41 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500118868724_1140039730",
      "id": "20170715-114108_2081955705",
      "dateCreated": "Jul 15, 2017 11:41:08 AM",
      "dateStarted": "Jul 23, 2017 3:49:42 PM",
      "dateFinished": "Jul 23, 2017 3:50:10 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Obtén las palabras usando el método split (split usa un espacio como delimitador por defecto)\npalabras \u003d lineas.flatMap(lambda x: x.split())\nprint(\"Número de particiones del RDD palabras \u003d {0}\".format(\n       palabras.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:19:24 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500310549988_1876915601",
      "id": "20170717-165549_824338572",
      "dateCreated": "Jul 17, 2017 4:55:49 PM",
      "dateStarted": "Jul 17, 2017 5:19:24 PM",
      "dateFinished": "Jul 17, 2017 5:19:24 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Reparticiono el RDD en 4 particiones       \npalabras2 \u003d palabras.coalesce(4)\nprint(\"Número de particiones del RDD palabras2 \u003d {0}\".format(\n       palabras2.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:19:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500310554533_1271336156",
      "id": "20170717-165554_447956162",
      "dateCreated": "Jul 17, 2017 4:55:54 PM",
      "dateStarted": "Jul 17, 2017 5:19:42 PM",
      "dateFinished": "Jul 17, 2017 5:19:43 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Toma una muestra aleatoria de palabras\nprint(palabras2.takeSample(False, 10))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:19:54 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500310556807_-942139590",
      "id": "20170717-165556_1006378560",
      "dateCreated": "Jul 17, 2017 4:55:56 PM",
      "dateStarted": "Jul 17, 2017 5:19:54 PM",
      "dateFinished": "Jul 17, 2017 5:20:08 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Salva el RDD palabras en varios ficheros de salida\n# (un fichero por partición)\npalabras2.saveAsTextFile(\"file:///tmp/salidatxt\")",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:20:52 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1500310558693_-1472323575",
      "id": "20170717-165558_378255199",
      "dateCreated": "Jul 17, 2017 4:55:58 PM",
      "dateStarted": "Jul 17, 2017 5:20:52 PM",
      "dateFinished": "Jul 17, 2017 5:21:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n# Ficheros de salida\nls -l /tmp/salidatxt\nhead /tmp/salidatxt/part-00002",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:21:30 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500119121338_-1509207091",
      "id": "20170715-114521_158888604",
      "dateCreated": "Jul 15, 2017 11:45:21 AM",
      "dateStarted": "Jul 17, 2017 5:21:30 PM",
      "dateFinished": "Jul 17, 2017 5:21:30 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lee los ficheros y devuelve un RDD clave/valor\n# clave-\u003enombre fichero, valor-\u003efichero completo\nprdd \u003d sc.wholeTextFiles(\"../datos/libros/p*.gz\")\nprint(\"Número de particiones del RDD prdd \u003d {0}\\n\".format(\n       prdd.getNumPartitions()))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:25:48 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500119206726_-79910899",
      "id": "20170715-114646_1618556398",
      "dateCreated": "Jul 15, 2017 11:46:46 AM",
      "dateStarted": "Jul 17, 2017 5:25:48 PM",
      "dateFinished": "Jul 17, 2017 5:25:48 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Obtiene un lista clave/valor\n# clave-\u003enombre fichero, valor-\u003enumero de palabras\nlista \u003d prdd.mapValues(lambda x: len(x.split())).collect()\n\nfor libro in lista:\n    print(\"El fichero {0:14s} tiene {1:6d} palabras\".format(\n           libro[0].split(\"/\")[-1], libro[1]))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:27:04 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500310633119_240117159",
      "id": "20170717-165713_317028187",
      "dateCreated": "Jul 17, 2017 4:57:13 PM",
      "dateStarted": "Jul 17, 2017 5:27:04 PM",
      "dateFinished": "Jul 17, 2017 5:27:07 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Ficheros Sequence\nFicheros clave/valor usados en Hadoop\n\n-   Sus elementos implementan la interfaz [`Writable`](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html)",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 4:38:23 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFicheros Sequence\u003c/h3\u003e\n\u003cp\u003eFicheros clave/valor usados en Hadoop\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSus elementos implementan la interfaz \u003ca href\u003d\"https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html\"\u003e\u003ccode\u003eWritable\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500121763411_710978477",
      "id": "20170715-122923_832495602",
      "dateCreated": "Jul 15, 2017 12:29:23 PM",
      "dateStarted": "Jul 17, 2017 4:38:20 PM",
      "dateFinished": "Jul 17, 2017 4:38:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nrdd \u003d sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)], 2)\n\n# Salvamos el RDD clave valor como fichero de secuencias\nrdd.saveAsSequenceFile(\"file:///tmp/sequenceoutdir2\")",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:31:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500308028578_1594710243",
      "id": "20170717-161348_41044279",
      "dateCreated": "Jul 17, 2017 4:13:48 PM",
      "dateStarted": "Jul 17, 2017 5:31:15 PM",
      "dateFinished": "Jul 17, 2017 5:31:16 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho \u0027Directorio de salida\u0027\nls -l /tmp/sequenceoutdir2\necho \u0027Intenta leer uno de los fichero\u0027\ncat /tmp/sequenceoutdir2/part-00000\necho\necho  \u0027Lee el fichero usando Hadoop\u0027\n/opt/hadoop/bin/hdfs dfs -text /tmp/sequenceoutdir2/part-00001",
      "user": "anonymous",
      "dateUpdated": "Jul 28, 2017 3:14:18 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500308596977_2135461404",
      "id": "20170717-162316_1566090953",
      "dateCreated": "Jul 17, 2017 4:23:16 PM",
      "dateStarted": "Jul 17, 2017 5:31:28 PM",
      "dateFinished": "Jul 17, 2017 5:31:33 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lo leemos en otro RDD\nrdd2 \u003d sc.sequenceFile(\"file:///tmp/sequenceoutdir2\", \n                       \"org.apache.hadoop.io.Text\", \n                       \"org.apache.hadoop.io.IntWritable\")\n                       \nprint(\"Contenido del RDD {0}\".format(rdd2.collect()))",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:33:06 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500308632369_-1217209863",
      "id": "20170717-162352_788726266",
      "dateCreated": "Jul 17, 2017 4:23:52 PM",
      "dateStarted": "Jul 17, 2017 5:33:06 PM",
      "dateFinished": "Jul 17, 2017 5:33:07 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Formatos de entrada/salida de Hadoop\nSpark puede interactuar con cualquier formato de fichero soportado por Hadoop \n- Soporta las APIs “vieja” y “nueva”\n- Permite acceder a otros tipos de almacenamiento (no fichero), p.e. HBase o MongoDB, a través de `saveAsHadoopDataSet` y/o `saveAsNewAPIHadoopDataSet`\n",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 4:38:38 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFormatos de entrada/salida de Hadoop\u003c/h3\u003e\n\u003cp\u003eSpark puede interactuar con cualquier formato de fichero soportado por Hadoop\u003cbr/\u003e- Soporta las APIs “vieja” y “nueva”\u003cbr/\u003e- Permite acceder a otros tipos de almacenamiento (no fichero), p.e. HBase o MongoDB, a través de \u003ccode\u003esaveAsHadoopDataSet\u003c/code\u003e y/o \u003ccode\u003esaveAsNewAPIHadoopDataSet\u003c/code\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500308263193_54497455",
      "id": "20170717-161743_1876544820",
      "dateCreated": "Jul 17, 2017 4:17:43 PM",
      "dateStarted": "Jul 17, 2017 4:19:37 PM",
      "dateFinished": "Jul 17, 2017 4:19:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Salvamos el RDD clave/valor como fichero de texto Hadoop (TextOutputFormat)\nrdd.saveAsNewAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n                            \"org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\",\n                            \"org.apache.hadoop.io.Text\",\n                            \"org.apache.hadoop.io.IntWritable\")\n                            ",
      "user": "anonymous",
      "dateUpdated": "Jul 17, 2017 5:49:17 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1500308377843_1015351731",
      "id": "20170717-161937_2014632436",
      "dateCreated": "Jul 17, 2017 4:19:37 PM",
      "dateStarted": "Jul 17, 2017 5:49:17 PM",
      "dateFinished": "Jul 17, 2017 5:49:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho \u0027Directorio de salida\u0027\nls -l /tmp/hadoopfileoutdir\ncat /tmp/hadoopfileoutdir/part-r-00001",
      "user": "anonymous",
      "dateUpdated": "Jul 18, 2017 3:46:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500308428501_-1296927861",
      "id": "20170717-162028_1424644460",
      "dateCreated": "Jul 17, 2017 4:20:28 PM",
      "dateStarted": "Jul 17, 2017 5:49:29 PM",
      "dateFinished": "Jul 17, 2017 5:49:29 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n# Lo leemos como fichero clave-valor Hadoop (KeyValueTextInputFormat)\nrdd3 \u003d sc.newAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n                          \"org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\",\n                          \"org.apache.hadoop.io.Text\",\n                          \"org.apache.hadoop.io.IntWritable\")\n                          \nprint(\"Contenido del RDD {0}\".format(rdd3.collect()))",
      "user": "anonymous",
      "dateUpdated": "Jul 18, 2017 3:46:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500308467782_-449448628",
      "id": "20170717-162107_429179099",
      "dateCreated": "Jul 17, 2017 4:21:07 PM",
      "dateStarted": "Jul 17, 2017 5:51:10 PM",
      "dateFinished": "Jul 17, 2017 5:51:10 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Tarea\n\nA partir del fichero apat63_99.txt, crear un conjunto de ficheros secuencia, que se almacenarán en el directorio apat63_99-seq. En estos ficheros, la clave tiene que ser el país (campo \"COUNTRY\") y el valor un string formado por el número de patente (campo \"PATENT\") y el año de concesión (campo \"GYEAR\") separados por una coma. Una línea de esto ficheros será, por ejemplo:\n\n\u003e BE \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 3070801,1963",
      "user": "anonymous",
      "dateUpdated": "Jul 23, 2017 11:46:40 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eTarea\u003c/h3\u003e\n\u003cp\u003eA partir del fichero apat63_99.txt, crear un conjunto de ficheros secuencia, que se almacenarán en el directorio apat63_99-seq. En estos ficheros, la clave tiene que ser el país (campo \u0026ldquo;COUNTRY\u0026rdquo;) y el valor un string formado por el número de patente (campo \u0026ldquo;PATENT\u0026rdquo;) y el año de concesión (campo \u0026ldquo;GYEAR\u0026rdquo;) separados por una coma. Una línea de esto ficheros será, por ejemplo:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003eBE \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; 3070801,1963\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500308840015_-1854275252",
      "id": "20170717-162720_379645653",
      "dateCreated": "Jul 17, 2017 4:27:20 PM",
      "dateStarted": "Jul 23, 2017 11:46:38 AM",
      "dateFinished": "Jul 23, 2017 11:46:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "Jul 19, 2017 7:31:50 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500492710808_-410749749",
      "id": "20170719-193150_192336955",
      "dateCreated": "Jul 19, 2017 7:31:50 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Tarea\n\nEscribir un programa Scala Spark que, a partir de los ficheros cite75_99.txt y apat63_99-seq, obtenga, para cada patente, el país, el año y el número de citas.\n\nUtilizar un *full outer join* para unir, por el campo común (el número de patente) los RDDs asociados a ambos ficheros.\n",
      "user": "anonymous",
      "dateUpdated": "Jul 23, 2017 3:52:26 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": false,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eTarea\u003c/h3\u003e\n\u003cp\u003eEscribir un programa Scala Spark que, a partir de los ficheros cite75_99.txt y apat63_99-seq, obtenga, para cada patente, el país, el año y el número de citas.\u003c/p\u003e\n\u003cp\u003eUtilizar un \u003cem\u003efull outer join\u003c/em\u003e para unir, por el campo común (el número de patente) los RDDs asociados a ambos ficheros.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1500808974135_-1497601070",
      "id": "20170723-112254_1990355459",
      "dateCreated": "Jul 23, 2017 11:22:54 AM",
      "dateStarted": "Jul 23, 2017 3:52:09 PM",
      "dateFinished": "Jul 23, 2017 3:52:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "Jul 23, 2017 12:03:14 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1500811394520_976160858",
      "id": "20170723-120314_1080217243",
      "dateCreated": "Jul 23, 2017 12:03:14 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Curso Spark/07 - Lectura y escritura de ficheros",
  "id": "2CF1T7FXW",
  "angularObjects": {
    "2CCY33GTB:shared_process": [],
    "2CCQCYKJM:shared_process": [],
    "2CD8VB8N5:shared_process": [],
    "2CEZ3N4ZK:shared_process": [],
    "2CCN184W1:shared_process": [],
    "2CCTDCCB9:shared_process": [],
    "2CBRCMJB7:shared_process": [],
    "2CDTHYD1N:shared_process": [],
    "2CD85MNWZ:shared_process": [],
    "2CEM88R8V:shared_process": [],
    "2CBSSQJJR:shared_process": [],
    "2CBG9JDC9:shared_process": [],
    "2CBJUH5Z5:shared_process": [],
    "2CET3TKHW:shared_process": [],
    "2CF1VUR2D:shared_process": [],
    "2CCZDD8PX:shared_process": [],
    "2CESEPECG:shared_process": [],
    "2CCZGPF6E:shared_process": []
  },
  "config": {},
  "info": {}
}